{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from typing import Union\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read variables from from env file\n",
    "load_dotenv()  \n",
    "OPENAI_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX=os.getenv(\"PINECONE_INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read Source Data Files, Data Cleaning, Splitting to chunks, Convert and Store it in Vector Database\n",
    "\n",
    "\n",
    "Data Source 1: Interface Functional Specification Document (PDF)\n",
    "\n",
    "Data Source 2: Interface Data Flow (PDF)\n",
    "\n",
    "Data Source 3: Production Support Issues & Resolutions (CSV)\n",
    "\n",
    "Data Source 4: Interface Mapping Sheet (XLSX)\n",
    "\n",
    "Data Source 5: Interface Architecture, Failure Modes & Error Handling Mechanism (PPTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Folder Path for Data Source Files\n",
    "SOURCE_FILES_PATH=\"C:\\\\Users\\\\ASHOKKUMAR KALIAPPAN\\\\Documents\\\\Ashok\\\\MSc_DataAnalytics\\\\Final_Project\\\\Doc\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for PDF extraction\n",
    "def read_data_from_pdf(FILE_PATH,EXTRACT_IMAGE_INPUT):\n",
    "    loader_dataflow = PyPDFLoader(FILE_PATH, extract_images=EXTRACT_IMAGE_INPUT)\n",
    "    pages_dataflow = loader_dataflow.load()\n",
    "    return pages_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data Source 1: Interface Functional Specification Document (PDF)\n",
    "extracted_data_fsd = read_data_from_pdf (SOURCE_FILES_PATH + \"Engineering_Datahub_Interface_FSD.pdf\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data Source 2: Interface Data Flow (PDF)\n",
    "extracted_data_interfaceflow = read_data_from_pdf (SOURCE_FILES_PATH + \"Engineering_Datahub_Interface_ProcessFlow.pdf\",True)\n",
    "## print(extracted_data_interfaceflow[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data Source 3: Production Support Issues & Resolutions (CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pdf():\n",
    "    ## lammatization, remove stop words and special chars (regex)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split Data into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to split document into chunks\n",
    "def chunk_data(docs,CHUNK_SIZE,CHUNK_OVERLAP):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE,chunk_overlap=CHUNK_OVERLAP)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=chunk_data(extracted_data_fsd,500,50)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Embeddings: Convert Chunks to Vectors and Store in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000020090138550>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000020090C02940>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Embedding using OpenAI - model='text-embedding-ada-002'\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_KEY)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the vector size / diemnsion of the embedding\n",
    "vector_size = embeddings.embed_query(\"This is a test sentence.\")\n",
    "\n",
    "len(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pinecone VectorDB initiation, prior to this created pinecone with dimension = 1536\n",
    "pc = Pinecone(api_key=\"024deca9-0a0a-4d33-b55d-8065e3e304c5\")\n",
    "index=pc.Index(\"supportassist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = PineconeVectorStore.from_documents(documents, embeddings, index_name=\"supportassist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: RAG Implementation - Leverage Open AI Model, take input query from user & retrieve similar vectors from Vector DB, pass query + similar vectors to get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful support assistant. Please respond to the IT application support executive queries\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity Retreive Results from Pinecone VectorDB\n",
    "def retrieve_query(query,k=5):\n",
    "    matching_results=pinecone_index.similarity_search(query,k=k)\n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI Model - gpt-3.5-turbo and chain creation\n",
    "llm=ChatOpenAI(openai_api_key=OPENAI_KEY,model_name=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "chain=load_qa_chain(llm,chain_type=\"stuff\")\n",
    "#question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "#rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search answers for the user query\n",
    "def retrieve_answers(query):\n",
    "    doc_search=retrieve_query(query)\n",
    "    response=chain.run(input_documents=doc_search,question=query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Engineering Datahub Interface Trigger is the Change Notice Audit Task Completion.\n"
     ]
    }
   ],
   "source": [
    "our_query = \"What is Engineering Datahub Interface Trigger?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data elements extracted from Change Notice are:\n",
      "\n",
      "1. Part\n",
      "2. Assembly\n",
      "3. Part / Assembly Drawing\n",
      "4. Engineering Specification Document\n"
     ]
    }
   ],
   "source": [
    "our_query = \"What are the data elements extracted from Change Notice?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-validations of the Engineering Datahub Interface include CA Validations and Engineering Datahub interface pre-validations.\n"
     ]
    }
   ],
   "source": [
    "our_query = \"What are the pre-validations of Engineering Datahub Interface?\"\n",
    "answer = retrieve_answers(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Chat UI enablement through streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
